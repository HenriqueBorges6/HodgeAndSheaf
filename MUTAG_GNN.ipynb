{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd92e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed fixada em 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Maths\\Projects\\HodgeAndSheaf\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import set_seed\n",
    "import torch\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e978e",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa038d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "dataset = TUDataset(root = \"/tmp/MUTAG\", name = \"MUTAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a2d21c",
   "metadata": {},
   "source": [
    "Dataset properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c12a75ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUTAG(188)\n",
      "number of graphs: 188\n",
      "number of classes: 2\n",
      "number of node features: 7\n",
      "number of edge features: 4\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(f\"number of graphs: {len(dataset)}\")\n",
    "print(f\"number of classes: {dataset.num_classes}\")\n",
    "print(f\"number of node features: {dataset.num_node_features}\")\n",
    "print(f\"number of edge features: {dataset.num_edge_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931d8a5b",
   "metadata": {},
   "source": [
    "dataset shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92a72e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3371, 7], edge_index=[2, 7442], edge_attr=[7442, 4], y=[188])\n"
     ]
    }
   ],
   "source": [
    "print(dataset._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c27f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "  Treino: 135 grafos\n",
      "  Validação: 34 grafos\n",
      "  Teste: 19 grafos\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "def create_data_splits(dataset, n_splits=10, batch_size=32, random_seed=42):\n",
    "    \"\"\"\n",
    "    Cria splits para validação cruzada k-fold estratificada.\n",
    "    \n",
    "    A estratificação garante que cada fold tenha aproximadamente a mesma\n",
    "    proporção de classes que o dataset completo. Isso é importante porque\n",
    "    temos um desbalanceamento de classes no MUTAG.\n",
    "    \"\"\"\n",
    "    # Extraindo os labels para estratificação\n",
    "    y = np.array([data.y.item() for data in dataset])\n",
    "    \n",
    "    # Criando o objeto de validação cruzada\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "    \n",
    "    # Lista para armazenar os loaders de cada fold\n",
    "    fold_loaders = []\n",
    "    \n",
    "    for fold_idx, (train_val_idx, test_idx) in enumerate(skf.split(np.zeros(len(y)), y)):\n",
    "        # Dividimos train_val_idx em treino e validação (80/20)\n",
    "        # Usamos estratificação novamente aqui\n",
    "        inner_skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "        train_idx, val_idx = next(inner_skf.split(\n",
    "            np.zeros(len(train_val_idx)), \n",
    "            y[train_val_idx]\n",
    "        ))\n",
    "        \n",
    "        # Convertendo índices relativos para absolutos\n",
    "        train_idx = train_val_idx[train_idx]\n",
    "        val_idx = train_val_idx[val_idx]\n",
    "        \n",
    "        # Criando subsets\n",
    "        train_dataset = dataset[train_idx.tolist()]\n",
    "        val_dataset = dataset[val_idx.tolist()]\n",
    "        test_dataset = dataset[test_idx.tolist()]\n",
    "        \n",
    "        # Criando DataLoaders\n",
    "        # O DataLoader do PyG automaticamente faz o batching de múltiplos grafos\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        fold_loaders.append({\n",
    "            'train': train_loader,\n",
    "            'val': val_loader,\n",
    "            'test': test_loader,\n",
    "            'train_size': len(train_dataset),\n",
    "            'val_size': len(val_dataset),\n",
    "            'test_size': len(test_dataset)\n",
    "        })\n",
    "        \n",
    "        if fold_idx == 0:\n",
    "            print(f'Fold {fold_idx + 1}:')\n",
    "            print(f'  Treino: {len(train_dataset)} grafos')\n",
    "            print(f'  Validação: {len(val_dataset)} grafos')\n",
    "            print(f'  Teste: {len(test_dataset)} grafos')\n",
    "    \n",
    "    return fold_loaders\n",
    "\n",
    "# Criando os splits\n",
    "fold_loaders = create_data_splits(dataset, n_splits=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc9130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphClassificationTrainer:\n",
    "    def __init__(self, model, fold_loaders, config, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.fold_loaders = fold_loaders\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Configurando otimizador\n",
    "        if config['optimizer'] == 'adam':\n",
    "            self.optimizer = torch.optim.Adam(\n",
    "                model.parameters(),\n",
    "                lr=config['lr'],\n",
    "                weight_decay=config['weight_decay']\n",
    "            )\n",
    "        \n",
    "        # Scheduler de learning rate\n",
    "        if config.get('scheduler') == 'step':\n",
    "            self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                self.optimizer,\n",
    "                step_size=config['step_size'],\n",
    "                gamma=config['gamma']\n",
    "            )\n",
    "        elif config.get('scheduler') == 'cosine':\n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                self.optimizer,\n",
    "                T_max=config['epochs']\n",
    "            )\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "        \n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Para early stopping\n",
    "        self.best_val_acc = 0\n",
    "        self.patience_counter = 0\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Treina por uma época completa, iterando sobre todos os batches.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Move o batch para o device apropriado (CPU ou GPU)\n",
    "            batch = batch.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            # Note que agora passamos batch.batch para indicar os limites dos grafos\n",
    "            out = self.model(batch.x, batch.edge_index, batch.batch)\n",
    "            \n",
    "            # Calcula a perda\n",
    "            loss = self.criterion(out, batch.y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping se configurado\n",
    "            if self.config.get('clip_grad'):\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(),\n",
    "                    self.config['clip_grad']\n",
    "                )\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Acumula estatísticas\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == batch.y).sum().item()\n",
    "            total += batch.num_graphs\n",
    "        \n",
    "        avg_loss = total_loss / total\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, loader):\n",
    "        \"\"\"\n",
    "        Avalia o modelo em um DataLoader específico.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in loader:\n",
    "            batch = batch.to(self.device)\n",
    "            out = self.model(batch.x, batch.edge_index, batch.batch)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == batch.y).sum().item()\n",
    "            total += batch.num_graphs\n",
    "        \n",
    "        return correct / total\n",
    "    \n",
    "    def train_fold(self, fold_idx):\n",
    "        \"\"\"\n",
    "        Treina o modelo em um fold específico da validação cruzada.\n",
    "        \"\"\"\n",
    "        print(f'\\n=== Treinando Fold {fold_idx + 1} ===')\n",
    "        \n",
    "        # Reseta o modelo para cada fold\n",
    "        self.model.apply(self._weight_reset)\n",
    "        \n",
    "        # Reseta otimizador e scheduler\n",
    "        self.__init__(self.model, self.fold_loaders, self.config, self.device)\n",
    "        \n",
    "        loaders = self.fold_loaders[fold_idx]\n",
    "        train_loader = loaders['train']\n",
    "        val_loader = loaders['val']\n",
    "        test_loader = loaders['test']\n",
    "        \n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_acc': [],\n",
    "            'test_acc': []\n",
    "        }\n",
    "        \n",
    "        self.best_val_acc = 0\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "        for epoch in range(1, self.config['epochs'] + 1):\n",
    "            # Treina por uma época\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            \n",
    "            # Avalia em validação e teste\n",
    "            val_acc = self.evaluate(val_loader)\n",
    "            test_acc = self.evaluate(test_loader)\n",
    "            \n",
    "            # Guarda histórico\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            history['test_acc'].append(test_acc)\n",
    "            \n",
    "            # Atualiza learning rate\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.config.get('early_stopping'):\n",
    "                if val_acc > self.best_val_acc:\n",
    "                    self.best_val_acc = val_acc\n",
    "                    self.best_model_state = self.model.state_dict().copy()\n",
    "                    self.patience_counter = 0\n",
    "                else:\n",
    "                    self.patience_counter += 1\n",
    "                \n",
    "                if self.patience_counter >= self.config['patience']:\n",
    "                    print(f'Early stopping na época {epoch}')\n",
    "                    self.model.load_state_dict(self.best_model_state)\n",
    "                    break\n",
    "            \n",
    "            # Log periódico\n",
    "            if epoch % self.config['log_interval'] == 0:\n",
    "                print(f'Época {epoch:03d}: Loss={train_loss:.4f}, '\n",
    "                      f'Train={train_acc:.4f}, Val={val_acc:.4f}, Test={test_acc:.4f}')\n",
    "        \n",
    "        # Avaliação final no melhor modelo\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "        \n",
    "        final_val_acc = self.evaluate(val_loader)\n",
    "        final_test_acc = self.evaluate(test_loader)\n",
    "        \n",
    "        print(f'Melhor Val Acc: {final_val_acc:.4f}')\n",
    "        print(f'Test Acc final: {final_test_acc:.4f}')\n",
    "        \n",
    "        return history, final_val_acc, final_test_acc\n",
    "    \n",
    "    def train_all_folds(self):\n",
    "        \"\"\"\n",
    "        Executa validação cruzada completa em todos os folds.\n",
    "        \"\"\"\n",
    "        val_accs = []\n",
    "        test_accs = []\n",
    "        all_histories = []\n",
    "        \n",
    "        for fold_idx in range(len(self.fold_loaders)):\n",
    "            history, val_acc, test_acc = self.train_fold(fold_idx)\n",
    "            val_accs.append(val_acc)\n",
    "            test_accs.append(test_acc)\n",
    "            all_histories.append(history)\n",
    "        \n",
    "        # Calcula estatísticas agregadas\n",
    "        mean_val_acc = np.mean(val_accs)\n",
    "        std_val_acc = np.std(val_accs)\n",
    "        mean_test_acc = np.mean(test_accs)\n",
    "        std_test_acc = np.std(test_accs)\n",
    "        \n",
    "        print('\\n' + '='*50)\n",
    "        print('RESULTADOS DA VALIDAÇÃO CRUZADA')\n",
    "        print('='*50)\n",
    "        print(f'Validação: {mean_val_acc:.4f} ± {std_val_acc:.4f}')\n",
    "        print(f'Teste: {mean_test_acc:.4f} ± {std_test_acc:.4f}')\n",
    "        print('='*50)\n",
    "        \n",
    "        return {\n",
    "            'val_accs': val_accs,\n",
    "            'test_accs': test_accs,\n",
    "            'mean_val_acc': mean_val_acc,\n",
    "            'std_val_acc': std_val_acc,\n",
    "            'mean_test_acc': mean_test_acc,\n",
    "            'std_test_acc': std_test_acc,\n",
    "            'histories': all_histories\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def _weight_reset(m):\n",
    "        \"\"\"\n",
    "        Reseta os pesos de uma camada para reinicialização.\n",
    "        \"\"\"\n",
    "        if hasattr(m, 'reset_parameters'):\n",
    "            m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a107f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando device: cpu\n",
      "\n",
      "=== Treinando Fold 1 ===\n",
      "Época 020: Loss=0.5501, Train=0.7333, Val=0.8235, Test=0.8947\n",
      "Época 040: Loss=0.5158, Train=0.7185, Val=0.8235, Test=0.9474\n",
      "Early stopping na época 42\n",
      "Melhor Val Acc: 0.8235\n",
      "Test Acc final: 0.9474\n",
      "\n",
      "=== Treinando Fold 2 ===\n",
      "Época 020: Loss=0.5167, Train=0.7333, Val=0.6471, Test=0.7895\n",
      "Early stopping na época 39\n",
      "Melhor Val Acc: 0.6765\n",
      "Test Acc final: 0.7368\n",
      "\n",
      "=== Treinando Fold 3 ===\n",
      "Época 020: Loss=0.5103, Train=0.7556, Val=0.7647, Test=0.6842\n",
      "Época 040: Loss=0.4722, Train=0.7778, Val=0.8235, Test=0.6316\n",
      "Época 060: Loss=0.4642, Train=0.7704, Val=0.7941, Test=0.6316\n",
      "Early stopping na época 62\n",
      "Melhor Val Acc: 0.8235\n",
      "Test Acc final: 0.6316\n",
      "\n",
      "=== Treinando Fold 4 ===\n",
      "Época 020: Loss=0.5146, Train=0.7630, Val=0.6765, Test=0.7895\n",
      "Early stopping na época 26\n",
      "Melhor Val Acc: 0.6765\n",
      "Test Acc final: 0.8421\n",
      "\n",
      "=== Treinando Fold 5 ===\n",
      "Época 020: Loss=0.4804, Train=0.7407, Val=0.7941, Test=0.6316\n",
      "Early stopping na época 38\n",
      "Melhor Val Acc: 0.7941\n",
      "Test Acc final: 0.6316\n",
      "\n",
      "=== Treinando Fold 6 ===\n",
      "Época 020: Loss=0.4887, Train=0.7556, Val=0.7353, Test=0.7368\n",
      "Época 040: Loss=0.5009, Train=0.7481, Val=0.8235, Test=0.6316\n",
      "Early stopping na época 58\n",
      "Melhor Val Acc: 0.8235\n",
      "Test Acc final: 0.6842\n",
      "\n",
      "=== Treinando Fold 7 ===\n",
      "Época 020: Loss=0.5131, Train=0.7481, Val=0.6765, Test=0.7368\n",
      "Época 040: Loss=0.4958, Train=0.7481, Val=0.7059, Test=0.7368\n",
      "Época 060: Loss=0.4802, Train=0.7704, Val=0.7353, Test=0.7368\n",
      "Early stopping na época 71\n",
      "Melhor Val Acc: 0.7353\n",
      "Test Acc final: 0.7368\n",
      "\n",
      "=== Treinando Fold 8 ===\n",
      "Época 020: Loss=0.4828, Train=0.7556, Val=0.7647, Test=0.7368\n",
      "Época 040: Loss=0.4434, Train=0.8000, Val=0.7941, Test=0.7368\n",
      "Early stopping na época 49\n",
      "Melhor Val Acc: 0.7941\n",
      "Test Acc final: 0.7368\n",
      "\n",
      "=== Treinando Fold 9 ===\n",
      "Época 020: Loss=0.4988, Train=0.7794, Val=0.7647, Test=0.7222\n",
      "Época 040: Loss=0.5040, Train=0.7500, Val=0.8235, Test=0.7222\n",
      "Early stopping na época 49\n",
      "Melhor Val Acc: 0.8235\n",
      "Test Acc final: 0.7222\n",
      "\n",
      "=== Treinando Fold 10 ===\n",
      "Época 020: Loss=0.5160, Train=0.7426, Val=0.7941, Test=0.7222\n",
      "Early stopping na época 37\n",
      "Melhor Val Acc: 0.7941\n",
      "Test Acc final: 0.7222\n",
      "\n",
      "==================================================\n",
      "RESULTADOS DA VALIDAÇÃO CRUZADA\n",
      "==================================================\n",
      "Validação: 0.7765 ± 0.0561\n",
      "Teste: 0.7392 ± 0.0898\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Precisamos importar nossa classe ConfigurableGNN\n",
    "# (assumindo que você salvou o código anterior)\n",
    "import torch\n",
    "from models.GraphNeuralNetwork.GraphConvolutionalNetwork import GCN\n",
    "# Detecta se temos GPU disponível\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Usando device: {device}')\n",
    "\n",
    "# Configuração do modelo\n",
    "# Note que precisamos usar pooling='mean' ou 'max' para classificação de grafos\n",
    "model = GCN(\n",
    "    num_features=dataset.num_node_features,\n",
    "    num_classes=dataset.num_classes,\n",
    "    hidden_dims=[64, 64, 32],  # 3 camadas ocultas\n",
    "    conv_type='GCN',\n",
    "    activation='relu',\n",
    "    dropout=0.5,\n",
    "    batch_norm=True,\n",
    "    residual=False,  # Não usamos residual para rede de apenas 3 camadas\n",
    "    pooling='mean',  # CRUCIAL: pooling para agregar nós em representação do grafo\n",
    "    jk_mode=None\n",
    ")\n",
    "\n",
    "# Configuração do treinamento\n",
    "config = {\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 0.001,  # Learning rate um pouco menor para dataset pequeno\n",
    "    'weight_decay': 1e-4,\n",
    "    'scheduler': 'step',\n",
    "    'step_size': 50,\n",
    "    'gamma': 0.5,\n",
    "    'epochs': 200,\n",
    "    'clip_grad': 1.0,\n",
    "    'early_stopping': True,\n",
    "    'patience': 25,  # Paciência maior porque o dataset é pequeno\n",
    "    'log_interval': 20\n",
    "}\n",
    "\n",
    "# Criando o trainer\n",
    "trainer = GraphClassificationTrainer(model, fold_loaders, config, device=device)\n",
    "\n",
    "# Executando validação cruzada completa\n",
    "results = trainer.train_all_folds()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
